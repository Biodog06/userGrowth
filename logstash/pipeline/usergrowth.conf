input {
  beats {
    port => 5044
  }
}

filter {
  # 1. 第一层解析：解析 Filebeat 传来的原始 message
  # 如果 Filebeat 已经开启了 json.keys_under_root: true，这一步可能不需要，
  # 但为了保险起见，通常我们对 message 进行解析。
  json {
    source => "message"
    # 如果解析成功，删除原始的 message 字段以节省空间
    remove_field => ["message"]
  }

  # 2. 第二层解析：解析 content 字段中的 JSON 字符串
  if [content] {
    json {
      source => "content"
      # 将解析出来的数据（path, method, ip等）放在根目录下，方便检索
      remove_field => ["content"] # 解析成功后删除冗余的 content 字符串字段
    }
  }

  # 3. 第三层解析：解析 body 字段 (如果存在)
  if [body] {
     # 尝试解析 body，如果是 JSON 字符串
     json {
       source => "body"
       skip_on_invalid_json => true
       # 不删除 body，保留原始字符串以备查
       # remove_field => ["body"] 
     }
  }

  # 4. 时间处理
  # 利用日志里的 @timestamp 覆盖 Logstash 默认的读取时间
  # 你的日志格式已经是 ISO8601，Logstash 的 JSON filter 通常会自动处理 @timestamp
  # 如果没有自动处理，可以使用 date 插件（通常不需要，因为你的 key 就是 @timestamp）

  # 4. 索引路由逻辑 (关键步骤)
  # 根据 filebeat 传递的 log_topic 字段判断应该发往哪个索引名称的前缀

  if [log_topic] == "error" {
    mutate { add_field => { "[@metadata][index_prefix]" => "usergrowth-error" } }
  }
  else if [log_topic] == "access" {
    mutate { add_field => { "[@metadata][index_prefix]" => "usergrowth-access" } }
  }
  else if [log_topic] == "user" {
    mutate { add_field => { "[@metadata][index_prefix]" => "usergrowth-user" } }
  }
  else {
    # 默认情况
    mutate { add_field => { "[@metadata][index_prefix]" => "usergrowth-other" } }
  }
}

output {
  elasticsearch {
    # 修改为你的 ES 地址
    hosts => ["http://elasticsearch:9200"]
    # 如果有账号密码
    # user => "elastic"
    # password => "password"

    # 动态索引名称：前缀 (上面逻辑判断出来的) + 日期 (Logstash 自动按天生成)
    # 结果示例: usergrowth-access-2025.12.31
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"

    # 建议设置 action 为 create 或 index
    action => "create"
  }

  # 调试用：在控制台打印解析结果，正式上线可注释掉
  stdout { codec => rubydebug }
}